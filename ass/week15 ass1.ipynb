{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:**  \n",
    "- **Simple Linear Regression:** Involves one independent variable predicting a dependent variable (e.g., predicting house price based on square footage).  \n",
    "- **Multiple Linear Regression:** Involves multiple independent variables predicting a dependent variable (e.g., predicting house price based on square footage, number of bedrooms, and location).  \n",
    "\n",
    "**Q2:**  \n",
    "Assumptions of Linear Regression:  \n",
    "1. Linearity  \n",
    "2. Independence of errors  \n",
    "3. Homoscedasticity (constant variance of errors)  \n",
    "4. Normality of errors  \n",
    "5. No multicollinearity (for multiple regression)  \n",
    "\n",
    "Check assumptions using:  \n",
    "- Scatter plots (linearity, homoscedasticity)  \n",
    "- VIF (multicollinearity)  \n",
    "- Residual plots (normality, homoscedasticity)  \n",
    "\n",
    "**Q3:**  \n",
    "- **Slope (β1):** Change in the dependent variable for a one-unit change in the independent variable.  \n",
    "- **Intercept (β0):** Value of the dependent variable when all independent variables are zero.  \n",
    "\n",
    "Example: If a salary prediction model gives `Salary = 5000 + 200*Experience`, then every additional year of experience increases salary by $200.  \n",
    "\n",
    "**Q4:**  \n",
    "Gradient Descent is an optimization algorithm that minimizes the cost function by updating weights iteratively in the direction of the steepest descent. Used in ML to train models like linear regression and neural networks.  \n",
    "\n",
    "**Q5:**  \n",
    "- **Multiple Linear Regression Model:** `Y = β0 + β1X1 + β2X2 + ... + βnXn + ε`  \n",
    "- **Difference:** Simple linear regression has one independent variable, while multiple linear regression has two or more.  \n",
    "\n",
    "**Q6:**  \n",
    "- **Multicollinearity:** When independent variables are highly correlated, leading to unstable coefficient estimates.  \n",
    "- **Detection:** Check VIF (Variance Inflation Factor), correlation matrix.  \n",
    "- **Solution:** Remove highly correlated variables, use PCA, or regularization techniques like Ridge regression.  \n",
    "\n",
    "**Q7:**  \n",
    "- **Polynomial Regression:** Extends linear regression by adding polynomial terms (`Y = β0 + β1X + β2X² + ...`).  \n",
    "- **Difference:** Models nonlinear relationships, unlike standard linear regression, which assumes linearity.  \n",
    "\n",
    "**Q8:**  \n",
    "- **Advantages:** Captures nonlinear patterns, more flexible.  \n",
    "- **Disadvantages:** Overfitting, sensitive to noise, complex interpretation.  \n",
    "- **Use Cases:** When data shows a clear nonlinear trend (e.g., growth patterns, stock prices)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
