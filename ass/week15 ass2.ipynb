{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** R-squared (coefficient of determination) measures the proportion of variance in the dependent variable explained by the independent variables. It is calculated as:  \n",
    "\\[\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "\\]\n",
    "where \\( SS_{res} \\) is the sum of squared residuals, and \\( SS_{tot} \\) is the total sum of squares. A higher \\( R^2 \\) indicates a better fit.  \n",
    "\n",
    "**Q2:** Adjusted R-squared adjusts \\( R^2 \\) for the number of predictors, preventing overestimation of model performance. It is calculated as:  \n",
    "\\[\n",
    "R^2_{adj} = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n",
    "\\]\n",
    "where \\( n \\) is the number of observations and \\( p \\) is the number of predictors.  \n",
    "\n",
    "**Q3:** Adjusted R-squared is preferable when comparing models with different numbers of predictors, as it penalizes unnecessary variables.  \n",
    "\n",
    "**Q4:**  \n",
    "- **MSE (Mean Squared Error):** \\( \\frac{1}{n} \\sum (y_i - \\hat{y}_i)^2 \\) — Measures average squared error.  \n",
    "- **RMSE (Root Mean Squared Error):** \\( \\sqrt{MSE} \\) — Provides error in original units.  \n",
    "- **MAE (Mean Absolute Error):** \\( \\frac{1}{n} \\sum |y_i - \\hat{y}_i| \\) — Measures average absolute error.  \n",
    "\n",
    "**Q5:**  \n",
    "- **Advantages:**  \n",
    "  - RMSE gives higher weight to large errors.  \n",
    "  - MSE is useful for differentiating model performance.  \n",
    "  - MAE is robust to outliers.  \n",
    "- **Disadvantages:**  \n",
    "  - RMSE/MSE are sensitive to outliers.  \n",
    "  - MSE does not have the same units as the target variable.  \n",
    "  - MAE does not penalize large errors heavily.  \n",
    "\n",
    "**Q6:** Lasso (L1 regularization) adds \\( \\lambda \\sum |w_i| \\) to the cost function, forcing some coefficients to become zero. Unlike Ridge (L2 regularization), which shrinks coefficients but keeps them nonzero, Lasso performs feature selection.  \n",
    "\n",
    "**Q7:** Regularized models reduce overfitting by penalizing large coefficients. Example: In predicting house prices, Ridge prevents excessive weight on less important features, improving generalization.  \n",
    "\n",
    "**Q8:**  \n",
    "- **Limitations:**  \n",
    "  - May not work well if all predictors are important.  \n",
    "  - Regularization strength (\\(\\lambda\\)) needs fine-tuning.  \n",
    "  - Lasso can eliminate relevant variables if \\(\\lambda\\) is too high.  \n",
    "\n",
    "**Q9:** Model A (RMSE = 10) has higher error variance than Model B (MAE = 8). However, since RMSE penalizes larger errors more, a direct comparison is difficult. Choice depends on whether larger errors are critical.  \n",
    "\n",
    "**Q10:** The choice depends on the dataset. Ridge (Model A) is better if all features are relevant. Lasso (Model B) helps with feature selection. Trade-offs include bias-variance balance—Lasso can discard useful features, while Ridge keeps all but shrinks them."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
