{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** Ridge Regression is a linear regression model with L2 regularization, adding a penalty term \\( \\lambda \\sum w_i^2 \\) to the cost function. Unlike OLS, it reduces coefficient magnitudes to prevent overfitting but does not eliminate them.  \n",
    "\n",
    "**Q2:** Assumptions:  \n",
    "- Linearity between predictors and target.  \n",
    "- Independent and identically distributed (i.i.d.) errors.  \n",
    "- No perfect multicollinearity.  \n",
    "- Normally distributed residuals (for inference).  \n",
    "\n",
    "**Q3:** The tuning parameter \\( \\lambda \\) is selected using cross-validation, typically via grid search or automated methods like LassoCV.  \n",
    "\n",
    "**Q4:** No, Ridge does not perform feature selection because it shrinks coefficients but does not set them to zero.  \n",
    "\n",
    "**Q5:** Ridge handles multicollinearity well by shrinking correlated variable coefficients, preventing extreme values and reducing overfitting.  \n",
    "\n",
    "**Q6:** Yes, but categorical variables need to be encoded (e.g., one-hot encoding) before being used in Ridge Regression.  \n",
    "\n",
    "**Q7:** Coefficients indicate the relationship between predictors and the target, but they are biased due to shrinkage, meaning their magnitudes are reduced.  \n",
    "\n",
    "**Q8:** Yes, Ridge can be used for time-series forecasting, but feature engineering (e.g., lag variables) and handling autocorrelation are necessary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
